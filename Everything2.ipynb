{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'html'\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.html'):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            data = []\n",
    "            articles = soup.findAll('article', class_=\"tm-articles-list__item\")\n",
    "            for article in articles:\n",
    "                title = article.find('h2', class_=\"tm-title tm-title_h2\").text.strip()\n",
    "                try:\n",
    "                    description = article.find('div', class_=\"tm-article-body tm-article-snippet__lead\").text.strip()\n",
    "                except:\n",
    "                    description = ''\n",
    "                rating = article.find('div', class_=\"tm-votes-meter tm-data-icons__item\").find('span').text.strip()\n",
    "                field_activity = article.find('a', class_=\"tm-publication-hub__link\").text.strip()\n",
    "                date = article.find('a', class_=\"tm-article-datetime-published tm-article-datetime-published_link\").text.strip()\n",
    "                \n",
    "                data.append([title, description, rating, field_activity, date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"title\", \"description\", \"rating\", \"field_activity\", \"date\"]\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "df.to_csv('articles.csv', sep=';', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%pip install re\\n%pip install nltk\\n%pip install unicodedata\\n%pip install contractions\\n%pip install inflect\\n%pip install emoji\\n%pip install pymorphy2\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%pip install re\n",
    "%pip install nltk\n",
    "%pip install unicodedata\n",
    "%pip install contractions\n",
    "%pip install inflect\n",
    "%pip install emoji\n",
    "%pip install pymorphy2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\v1d3k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  Добрый день! Эта заметка — результат моих горе...   \n",
      "1  Привет, хабрачеловек! Мне довелось принимать у...   \n",
      "2  Вот тут пишут, что 1 ноября правительство Росс...   \n",
      "3  В наши дни бизнес стремится быть максимально к...   \n",
      "4  Всем известный закон Мёрфи гласит: «Если что-т...   \n",
      "\n",
      "                               processed_description  \n",
      "0  добрый день заметка — результат горестный разд...  \n",
      "1  привет хабрачеловек довестись принимать участи...  \n",
      "2  писать 1 ноябрь правительство россия утвердить...  \n",
      "3  наш день бизнес стремиться максимально клиенто...  \n",
      "4  весь известный закон мёрфь гласить « что-то пл...  \n",
      "                                               title  \\\n",
      "0  Зачем информационным технологиям нужны лингвисты?   \n",
      "1  Смех сквозь слёзы: Всеукраинский конкурс по ин...   \n",
      "2  Правительство России утвердило Стратегию разви...   \n",
      "3  Как улучшить клиентский опыт с помощью библиот...   \n",
      "4              Законы мира информационных технологий   \n",
      "\n",
      "                                     processed_title  \n",
      "0          информационный технология нужный лингвист  \n",
      "1  смех сквозь слеза всеукраинский конкурс информ...  \n",
      "2  правительство россия утвердить стратегия разви...  \n",
      "3  улучшить клиентский опыт помощь библиотека инф...  \n",
      "4                закон мир информационный технология  \n"
     ]
    }
   ],
   "source": [
    "morph = MorphAnalyzer()\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# Функция для токенизации, лемматизации и удаления стоп-слов\n",
    "def preprocess_text(text):\n",
    "    #удаление эмоджи\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = [word.lower() for word in word_tokenize(text) if re.match(r'\\w', word)]\n",
    "\n",
    "    # Лемматизация\n",
    "    lemmatized_tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    \n",
    "    # Удаление стоп-слов и пунктуации\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Применение функции к каждой ячейке в столбце 'articles'\n",
    "df['processed_description'] = df['description'].apply(preprocess_text)\n",
    "df['processed_title'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "# Вывод результата\n",
    "print(df[['description', 'processed_description']].head())\n",
    "print(df[['title', 'processed_title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('articles_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск ключевых слов/n-грамм. Векторизация текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'KeyFrequency'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Извлечение ключевых слов на основе частоты слов\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m key_words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKeyFrequency\u001b[49m()\u001b[38;5;241m.\u001b[39mkeys()[:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(key_words)\n\u001b[0;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПример текста для извлечения биграмм и триграмм на Python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'KeyFrequency'"
     ]
    }
   ],
   "source": [
    "text = \"Пример текста для извлечения ключевых слов на Python\"\n",
    "words = set(nltk.word_tokenize(text))\n",
    "\n",
    "# Извлечение ключевых слов на основе частоты слов\n",
    "key_words = nltk.KeyFrequency().keys()[:10]\n",
    "print(key_words)\n",
    "\n",
    "text = \"Пример текста для извлечения биграмм и триграмм на Python\"\n",
    "tokens = word_tokenize(text)\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "# Извлечение биграмм\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "finder.apply_freq_filter(2) # Отфильтровать биграммы, которые встречаются менее 2 раз\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 10) # Извлечь 10 наиболее значимых биграмм\n",
    "print(bigrams)\n",
    "\n",
    "# Извлечение триграмм\n",
    "finder = TrigramCollocationFinder.from_words(tokens)\n",
    "finder.apply_freq_filter(2) # Отфильтровать триграммы, которые встречаются менее 2 раз\n",
    "trigrams = finder.nbest(trigram_measures.pmi, 10) # Извлечь 10 наиболее значимых триграмм\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разведочный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка отчета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
